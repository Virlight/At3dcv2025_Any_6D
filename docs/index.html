<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Project at TUM: Visual Grounded Open-Vocabulary Object Pose Estimation">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="Project at TUM on single-RGB 6D object pose estimation: VGGT depth, 2024–2025 image-to-3D mesh benchmarking, and Any6D with RGB+predicted depth; evaluated on HouseCat6D.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="6D pose estimation, single-view, RGB-only, depth estimation, VGGT, image-to-3D, mesh generation, Any6D, HouseCat6D, computer vision, robotics">
  <!-- TODO: List all authors -->
  <meta name="author" content="Yung Jhang Hou, Haoliang Huang, Namoz Ostonaev">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Technical University of Munich">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Project at TUM: Visual Grounded Open-Vocabulary Object Pose Estimation">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="Project at TUM on single-RGB 6D object pose estimation: VGGT depth, 2024–2025 image-to-3D mesh benchmarking, and Any6D with RGB+predicted depth; evaluated on HouseCat6D.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://virlight.github.io/At3dcv2025_Any_6D/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Project at TUM: Visual Grounded Open-Vocabulary Object Pose Estimation - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Yung Jhang Hou, Haoliang Huang, Namoz Ostonaev">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="6D pose estimation">
  <meta property="article:tag" content="single-view">
  <meta property="article:tag" content="RGB-only">
  <meta property="article:tag" content="depth estimation">
  <meta property="article:tag" content="VGGT">
  <meta property="article:tag" content="image-to-3D">
  <meta property="article:tag" content="mesh generation">
  <meta property="article:tag" content="Any6D">
  <meta property="article:tag" content="HouseCat6D">
  <meta property="article:tag" content="computer vision">
  <meta property="article:tag" content="robotics">

  <!-- Twitter -->
  <!-- <meta name="twitter:card" content="summary_large_image"> -->
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <!-- <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE"> -->
  <!-- TODO: Replace with first author's Twitter handle -->
  <!-- <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE"> -->
  <!-- TODO: Same as paper title above -->
  <!-- <meta name="twitter:title" content="PAPER_TITLE"> -->
  <!-- TODO: Same as description above -->
  <!-- <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS"> -->
  <!-- TODO: Same as social preview image above -->
  <!-- <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview"> -->

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
<title>Project at TUM: Visual Grounded Open-Vocabulary Object Pose Estimation</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/camp.ico">
  <link rel="apple-touch-icon" href="static/images/camp.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <!-- <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        TODO: Replace with your lab's related works
        <a href="https://arxiv.org/abs/PAPER_ID_1" class="work-item" target="_blank">
          <div class="work-info">
            TODO: Replace with actual paper title
            <h5>Paper Title 1</h5>
            TODO: Replace with brief description
            <p>Brief description of the work and its main contribution.</p>
            TODO: Replace with venue and year
            <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        TODO: Add more related works or remove extra items
        <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 2</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div> -->

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">Project at TUM: Visual Grounded Open-Vocabulary Object Pose Estimation</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Yung Jhang Hou </a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Haoliang Huang</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Namoz Ostonaev</a><sup>*</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block">
                      Chair for Computer Aided Medical Procedures (CAMP)<br>
                      Advanced Topics in 3D Computer Vision (SS 2025)<br>
                      Supervised by: Boody Elskhawy, Weihang Li
                    </span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <span class="link-block">
                      <a href="static/pdfs/AT3DCV-6D-Pose.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Slides</span>
                    </a>
                  </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/Virlight/At3dcv2025_Any_6D" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video -->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="teaser-video" autoplay controls muted loop height="100%" preload="metadata" playsinline>
        <source src="static/videos/banner_video.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <h2 class="subtitle has-text-centered">
        Teaser showcasing our method and results.
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Motivated by the growing deployment of mobile robots in everyday environments, 
            we build and evaluate a pipeline for single-view 6D object pose estimation from one RGB image. 
            The pipeline has three modules: (i) depth estimation with VGGT; 
            (ii) image-to-mesh generation, where we benchmark several strong 2025 image-to-3D generators under a common preprocessing and scoring protocol to obtain meshes as shape priors; 
            and (iii) pose estimation with Any6D, modified to take RGB + VGGT-predicted depth in place of the original RGB-D input. 
            We run experiments on HouseCat6D. 
            Equal contribution: Haoliang Huang for image-to-mesh generation; Yung Jhang Hou for evaluation design and metrics; Namoz Ostonaev for depth estimation. 
            Although responsibilities were split, we co-designed the overall architecture and mutually reviewed all components, assembling them into a working pipeline.
            Through this project, we developed a deeper understanding of 3D computer vision as a whole—covering depth estimation, 
            novel view synthesis, mesh and texture generation, object association, pose estimation, 3D model and 6D pose evaluation. 
            We surveyed common practices, recent methods and neural models, and emerging trends; 
            identified typical issues and performance bottlenecks; 
            and, critically, learned how to assemble an end-to-end system by orchestrating existing methods (both classical and neural) to achieve the desired functionality. 
            We also studied evaluation methodology, including metrics and standard datasets, which enabled us to design a rigorous benchmark. 
            These insights will lay a solid foundation for our future research in this field.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Method Overview / Detailed Pipeline -->
<section class="section" id="pipeline">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Method Overview</h2>
    <figure>
      <img
        src="static/images/pipeline.png" 
        alt="Detailed pipeline: VGGT depth, image-to-3D generation and alignment, Any6D pose & refinement"
        loading="lazy" decoding="async"
        style="border-radius:12px; box-shadow:0 10px 30px rgba(0,0,0,.08);">
      <figcaption class="has-text-centered has-text-grey is-size-6" style="margin-top:.5rem;">
        Figure: Updated pipeline — VGGT depth → image-to-3D mesh (with alignment) → Any6D pose & refinement.
      </figcaption>
    </figure>
  </div>
</section>

<!-- Teaser image (Simple Pipeline) -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image">
        <img
          src="static/images/simplified-pipeline.png" 
          alt="Simple pipeline: RGB → Mesh generation / Depth estimation → Any6D → 6D pose"
          loading="eager" decoding="async"
          style="border-radius:12px; box-shadow:0 10px 30px rgba(0,0,0,.10);">
        <figcaption class="has-text-centered has-text-grey is-size-6" style="margin-top:.5rem;">
          Figure: Simplified Pipeline at a glance — RGB → mesh & depth → Any6D → 6D pose.
        </figcaption>
      </figure>
    </div>
  </div>
</section>

<!-- Part 1: Depth Estimation (text block) -->
<section class="section">
  <div class="container is-max-desktop">
    <h3 class="title is-3">Part 1 — Depth Estimation (Namoz Ostonaev)</h3>
    <div class="content has-text-justified">
      <p>
        We predict dense metric depth from a single RGB image using <strong>VGGT</strong>.
        We examined input normalization and scale alignment, and validated robustness across typical indoor scenes in HouseCat6D.
      </p>
      <ul>
        <li><strong>Backbone:</strong> VGGT (single-image depth)</li>
        <li><strong>Input:</strong> single RGB image</li>
        <li><strong>Output:</strong> depth map used downstream by pose estimation</li>
      </ul>
    </div>
  </div>
</section>

<!-- Part 1 — Single Image, same width as carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">  <!-- 注意：不要 is-max-desktop -->
      <figure class="image fullwidth-figure">
        <img src="static/images/depth-estimation.png"
             alt="Depth estimation result (VGGT) on sample image"
             loading="lazy" decoding="async"
             style="border-radius:12px; box-shadow:0 10px 30px rgba(0,0,0,.08);">
        <figcaption class="has-text-centered is-size-6" style="margin-top:.5rem;">
          Figure: Given housecat6D dataset, the first row are the target images, second row are the estimated depths by VGGT, and the third row are the ground truth depths.
        </figcaption>
      </figure>
    </div>
  </div>
</section>

<!-- Part 2: Image-to-Mesh Generation (text block) -->
<section class="section">
  <div class="container is-max-desktop">
    <h3 class="title is-3">Part 2 — Image-to-Mesh Generation (Haoliang Huang)</h3>
    <div class="content has-text-justified">
      <p>
        We benchmark a set of <strong>image-to-3D generators released in 2024–2025</strong> under a unified
        preprocessing and evaluation protocol to produce object meshes that serve as shape priors for pose estimation.
        The goal of this stage is to perform a standardized comparison and <strong>select the best-performing generator and preprocessing strategy</strong>
        for downstream pose estimation. 
        Predicted 3D models are rigidly aligned to the Any6D coordinate convention, and their geometric quality is
        measured with <strong>Chamfer Distance</strong> using the Any6D assignment policy.
      </p>
      <ul>
        <li><strong>Input:</strong> single RGB image</li>
        <li><strong>Output:</strong> object meshes with textures</li>
        <li><strong>Evaluation:</strong> Chamfer Distance after rigid alignment (Any6D assignment policy)</li>
      </ul>
    </div>
  </div>
</section>

<!-- Part 2 — Single Image -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">  
      <figure class="image fullwidth-figure">
        <img src="static/images/generation-pipeline.png"
             alt="Image-to-Mesh generation pipeline"
             loading="lazy" decoding="async"
             style="border-radius:12px; box-shadow:0 10px 30px rgba(0,0,0,.10);">
        <figcaption class="has-text-centered has-text-grey is-size-6" style="margin-top:.5rem;">
          Figure: Single-image-to-mesh generation pipeline overview.
        </figcaption>
      </figure>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="content has-text-justified">
      <p>
        We take <strong>InstantMesh</strong> as the <em>baseline</em>, because it is the default generator in the
        <strong>Any6D</strong> framework we build upon. A quick diagnosis shows that the main performance bottleneck
        lies in the <em>novel-view synthesis stage</em>. We therefore run two rounds of comparisons under a unified
        preprocessing and evaluation protocol.
      </p>
      <ul>
        <li><strong>Round 1 — Replace novel-view synthesis only:</strong> keep the InstantMesh mesher (stage 2) and
            swap its finetuned Zero123++ (stage 1) with stronger multi-view generators.  
            <em>Observation:</em> none surpassed the baseline, mainly due to view inconsistency/coverage issues.</li>
        <li><strong>Round 2 — Full replacement of InstantMesh:</strong>
          we evaluate (A) two-stage pipelines (multi-view generation + meshing) and (B) end-to-end single-image-to-mesh models.</li>
      </ul>
      <p class="is-size-6 has-text-grey">
        <strong>Goal:</strong> a standardized comparison to <strong>select the best generator + preprocessing</strong> for downstream pose estimation.
      </p>
    </div>
  </div>
</section>

<!-- Round 1: replace stage-1 only -->
<section class="section">
  <div class="container">
    <h4 class="title is-4">Round 1 — Replace novel-view synthesis (keep InstantMesh mesher)</h4>
    <div id="results-carousel-r1" class="carousel results-carousel">
      <div class="item">
        <img src="static/images/multiview1.png" alt="Multi-view generation comparison" loading="lazy"/>
        <h2 class="subtitle has-text-centered">Multi-view generators (e.g., EscherNet, SV3D, MVGenMaster)</h2>
      </div>
      <div class="item">
        <img src="static/images/multiview2.png" alt="Era3D + InstantMesh meshing" loading="lazy"/>
        <h2 class="subtitle has-text-centered">Era3D (views) + InstantMesh (meshing)</h2>
      </div>
    </div>
    <p class="has-text-centered has-text-grey is-size-6" style="margin-top:.5rem;">
      Result: replacing only stage 1 did not outperform InstantMesh baseline due to view inconsistency/coverage.
    </p>
  </div>
</section>

<!-- Round 2: full replacement -->
<section class="section">
  <div class="container">
    <h4 class="title is-4">Round 2 — Full replacement of InstantMesh</h4>
    <p class="is-size-6">
      We compare (A) two-stage pipelines and (B) end-to-end models:
    </p>
    <div id="results-carousel-r2" class="carousel results-carousel">
      <!-- Two-stage pipelines -->
      <div class="item">
        <img src="static/images/end2end1.png" alt="InstantMesh baseline" loading="lazy"/>
        <h2 class="subtitle has-text-centered">InstantMesh (baseline, 2024)</h2>
      </div>
      <div class="item">
        <img src="static/images/end2end2.png" alt="CraftMann two-stage model" loading="lazy"/>
        <h2 class="subtitle has-text-centered">CraftMann (two-stage, 2025)</h2>
      </div>

      <!-- End-to-end models -->
      <div class="item">
        <img src="static/images/end2end3.png" alt="Hunyuan3D-2.5 end-to-end" loading="lazy"/>
        <h2 class="subtitle has-text-centered">Hunyuan3D 2.5 (end-to-end, 2025)</h2>
      </div>
      <div class="item">
        <img src="static/images/end2end4.png" alt="Hi3DGen end-to-end" loading="lazy"/>
        <h2 class="subtitle has-text-centered">Hi3DGen (end-to-end, 2025)</h2>
      </div>
      <div class="item">
        <img src="static/images/end2end5.png" alt="TRELLIS end-to-end" loading="lazy"/>
        <h2 class="subtitle has-text-centered">TRELLIS (end-to-end, 2025)</h2>
      </div>
      <div class="item">
        <img src="static/images/end2end6.png" alt="MIDI-3D end-to-end" loading="lazy"/>
        <h2 class="subtitle has-text-centered">MIDI-3D (end-to-end, 2025)</h2>
      </div>
    </div>
  </div>
</section>


<!-- Part 3: Pose Estimation & Evaluation (Yung Jhang Hou) -->
<section class="section">
  <div class="container is-max-desktop">
    <h3 class="title is-3">Part 3 — Pose Estimation & Evaluation (Yung Jhang Hou)</h3>
    <div class="content has-text-justified">
      <p>
        We adapt <strong>Any6D</strong> to take <strong>RGB + VGGT-derived depth</strong> instead of RGB-D, enabling single-image inference without a depth sensor. To assess sensitivity to mesh priors, we design a two-round study on <strong>HouseCat6D</strong> using <strong>15 representative objects</strong> spanning diverse shapes and materials.
      </p>

      <p><strong>Round&nbsp;1 (generator only).</strong> Using ground-truth masks and an identical preprocessing pipeline, we compare image-to-3D generators. Based on this standardized setting, we select two candidates—<strong>Hunyuan3D&nbsp;2.5</strong> and <strong>TRELLIS</strong>—for the next round.</p>

      <p><strong>Round&nbsp;2 (realistic pipeline).</strong> We fix the generator (each candidate in turn) and evaluate the full RGB-only pipeline with <em>segmentation&nbsp;+ preprocessing</em> to obtain meshes in a realistic usage scenario.</p>

      <p><strong>Metric choice.</strong> Geometry quality is measured by <strong>Chamfer Distance</strong> after rigid alignment using Any6D’s assignment policy. Instead of averaging Chamfer Distance, we report the <strong>Top-1 win count</strong> across objects/views, because we observe that Any6D’s stochastic, learned scoring/assignment sometimes yields <em>scores that poorly correlate with visual quality</em> on HouseCat6D (and occasional assignment failures), making means unreliable.</p>

      <ul>
        <li><strong>Framework:</strong> Any6D (modified input interface)</li>
        <li><strong>Metrics:</strong> Chamfer Distance (Top-1 wins), and standard pose metrics (ADD(-S), 2D projection, R/T errors)</li>
        <li><strong>Key finding:</strong> <strong>TRELLIS</strong> produces the strongest meshes among tested models in our setting.</li>
      </ul>

    </div>
  </div>
</section>

<!-- Generationn Estimation -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
          <img src="static/images/evaluation1.png" alt="Round 1: generator-only comparison with GT masks and identical preprocessing" loading="lazy"/>
          <h2 class="subtitle has-text-centered">
            Round 1 — Generator only (GT masks + same preprocessing)
          </h2>
        </div>

        <div class="item">
          <img src="static/images/evaluation2.png" alt="Round 2: realistic pipeline with segmentation and preprocessing" loading="lazy"/>
          <h2 class="subtitle has-text-centered">
            Round 2 — Hunyuan3D Realistic pipeline (segmentation + preprocessing)
          </h2>
        </div>

        <div class="item">
          <img src="static/images/evaluation3.png" alt="Top-1 Chamfer Distance wins across objects/views" loading="lazy"/>
          <h2 class="subtitle has-text-centered">
            Round 2 — TRELLIS Realistic pipeline (segmentation + preprocessing) — Top-1 Chamfer Distance wins (per object/view)
          </h2>
        </div>
  </div>
</div>
</div>
</section>
<!-- End Generation Estimation -->

<!-- Pose estimation: limitation & next steps -->
<section class="section">
  <div class="container is-max-desktop">
    <p class="content has-text-justified">
      <p>
      <strong>Pose estimation – current limitation.</strong>
      <strong>HouseCat6D</strong>, Any6D’s scoring/assignment is unstable; as a result, even the
      <em>InstantMesh</em> baseline underperforms. A more robust assignment strategy is likely required
      before a conclusive pose evaluation. At present, the system can run single-RGB-image → 6D pose,
      meeting our semester challenge, but accuracy is below expectation and scripts are not yet fully integrated. 
      </p>
      <strong>Next:</strong> replace/upgrade the assignment module and consolidate the components into a single pipeline.
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <!-- 两列整体居中 + 垂直对齐 -->
    <div class="columns is-centered is-vcentered is-variable is-5">

      <!-- 左列：5/12 宽 -->
      <div class="column is-5">
        <figure class="image">
          <!-- 固定高度的包裹容器，居中图片 -->
          <div class="is-flex is-justify-content-center is-align-items-center"
               style="height:420px;">
            <img
              src="static/images/assignment-scoring.png"
              alt="Any6D scoring/assignment instability on HouseCat6D"
              loading="lazy" decoding="async"
              style="max-height:100%; width:auto; object-fit:contain; object-position:center;
                     border-radius:12px; box-shadow:0 8px 24px rgba(0,0,0,.08);">
          </div>
          <figcaption class="has-text-centered has-text-grey is-size-6" style="margin-top:.5rem;">
            Any6D scoring / assignment on HouseCat6D — unstable and weak correlation with geometry quality and frequent failures.
          </figcaption>
        </figure>
      </div>

      <!-- 右列：7/12 宽 -->
      <div class="column is-7">
        <figure class="image">
          <div class="is-flex is-justify-content-center is-align-items-center"
               style="height:420px;">
            <img
              src="static/images/pose-evaluation.png"
              alt="Pose accuracy on HouseCat6D with current assignment"
              loading="lazy" decoding="async"
              style="max-height:100%; width:auto; object-fit:contain; object-position:center;
                     border-radius:12px; box-shadow:0 8px 24px rgba(0,0,0,.08);">
          </div>
          <figcaption class="has-text-centered has-text-grey is-size-6" style="margin-top:.5rem;">
            Pose accuracy under current assignment — InstantMesh baseline on HouseCat6D.
          </figcaption>
        </figure>
      </div>

    </div>
  </div>
</section>

<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- TODO: Replace with your YouTube video ID -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div> -->
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          TODO: Add poster image for better preview
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
            Your video file here
            <source src="static/videos/carousel1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          TODO: Add poster image for better preview
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
            Your video file here
            <source src="static/videos/carousel2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          TODO: Add poster image for better preview
          <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
            Your video file here
            <source src="static/videos/carousel3.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      TODO: Replace with your poster PDF
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->



<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{YourPaperKey2024,
  title={Your Paper Title Here},
  author={First Author and Second Author and Third Author},
  journal={Conference/Journal Name},
  year={2024},
  url={https://your-domain.com/your-project-page}
}</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
